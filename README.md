# ML Evaluation Framework

This directory contains the machine learning evaluation pipeline for the Project360 project. It automates the process of generating programming assignments from datasets, producing correct and explicitly "buggy" student solutions, grading these submissions using our custom MQL approach versus a baseline ChatGPT and Gemini approach, and analyzing the comparative results.

## Directory Structure

- `datasets/`: Contains generated JSON datasets storing the assignment questions, reference codes, buggy codes, and reference MQLs (e.g., `ml_assignments_100.json`).
- `results/`: Stores raw `.jsonl` evaluation outputs and aggregated `.txt` analysis reports.


## Key Scripts

### 1. `generate_ml_dataset.py`
This script takes local CSV datasets (found in `media/mldataset_files`) and uses an LLM to generate programming assignments dynamically. 
For each dataset, it generates:
- A machine learning question (e.g., Regression, Classification).
- The correct reference solution in Python (using `pandas`/`sklearn`).
- A dynamically generated "buggy" solution with semantic/logical flaws.
- The corresponding MQL queries representing the correct solution (extracted via the MQL pipeline).

**Usage:**
```bash
python ml_eval_framework/generate_ml_dataset.py
```

### 2. `run_ml_evaluation.py`
This script executes the evaluation pipeline against the generated assignments found in `%mldataset%.json`. 
It evaluates both the "correct" and "buggy" code solutions using two distinct methods to provide a basis for comparison:
- **Baseline GPT Grading**: Passes the question, student code, and reference code directly to ChatGPT.
- **MQL Pipeline Grading**: Leverages `convert_code_to_mql_pipeline` and `grade_mql` (combining contextual data, instructor MQL, and student-produced components).

Scores and feedback are appended sequentially to the `.jsonl` logs inside the `results/` folder.

**Usage:**
```bash
python ml_eval_framework/run_ml_evaluation.py
```

### 3. `analyze_ml_results.py`
Analyzes the results generated by the evaluation script. It generates a comparative text report detailing:
- The True Positive Rate (how accurately both systems recognized the correct code).
- The True Negative Rate / Rejection Rate (how accurately both systems recognized and downgraded the buggy code).
- Average scores and comparative feedback samples.

**Usage:**
```bash
python ml_eval_framework/analyze_ml_results.py
```

### Additional Scripts
- **`compare_models.py`**: Further comparisons or specific model configurations for analyzing performance.
- **`create_plots.py`**: Generates visual plots for the evaluation results.
- **`test_gemini.py` / `quick_gemini_buggy.py`**: Sandbox/test scripts for experimenting with Gemini prompts. 

